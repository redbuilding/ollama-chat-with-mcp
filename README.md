# ğŸ” ğŸ¤– ğŸŒ Ollama Chat with MCP (Refactored)



A powerful demonstration of integrating local LLMs with real-time web search capabilities and database
querying using the Model Context Protocol (MCP), featuring a modern web interface and persistent
conversations.



## Overview



Ollama Chat with MCP showcases how to extend a local language model's capabilities through tool use.
This application combines the power of locally running LLMs via Ollama with up-to-date web search
functionality and database querying, all managed through a robust backend and a user-friendly React
frontend. Conversations are persisted using MongoDB.



The project consists of several key components:

- **Backend (FastAPI)**: Manages chat logic, Ollama interactions, MCP service communication (including
starting and managing MCP services like web search and SQL querying), and conversation persistence.

- **Frontend (React)**: A modern, responsive web interface for users to interact with the chat
application.

- **MCP Web Search Server Module**: Provides web search functionality using the Serper.dev API, run as a
service by the backend.

- **MCP SQL Server Module**: Provides a tool to query a MySQL database, run as a service by the backend.

- **MongoDB**: Stores conversation history and user data.



This architecture demonstrates how MCP enables local models to access external tools and data sources,
significantly enhancing their capabilities, now with a more scalable and feature-rich setup.



## Features



- ğŸ” **Web-enhanced chat**: Access real-time web search results during conversation.

- ğŸ’¾ **Persistent Conversations**: Chat history is saved in MongoDB, allowing users to resume
conversations.

- ğŸ§  **Local model execution**: Uses Ollama to run models entirely on your own hardware.

- ğŸ”Œ **MCP integration**: Demonstrates practical implementation of the Model Context Protocol for
multiple tools, managed by the main backend.

- ğŸ’» **Modern Web Interface**: Built with React for a responsive and interactive user experience.

- ğŸ“Š **Structured search results**: Clean formatting of web search data for optimal context.

- âš™ï¸ **Backend API**: FastAPI backend providing robust API endpoints for chat and conversation
management.

- ğŸ—ƒï¸ **SQL Querying Tool**: MCP tool for interacting with a MySQL database, with improved schema handling
and query retry logic.

- ğŸ”„ **Conversation Management**: List, rename, and delete conversations.



## Requirements



- Python 3.11+

- Node.js (v18+) and npm/yarn for the frontend

- [Ollama](https://ollama.com/) installed and running locally

- A [Serper.dev](https://serper.dev/) API key (free tier available)

- MongoDB instance (local or cloud)

- MySQL server (optional, for the SQL querying tool)

- Internet connection for web searches and package downloads



## Installation



1.  **Clone the repository:**

    ```bash

    git clone https://github.com/redbuilding/ollama-chat-with-mcp.git

    cd ollama-chat-with-mcp

    ```



2.  **Set up Backend:**

    *   Navigate to the backend directory:

        ```bash

        cd backend

        ```

    *   Install Python dependencies (ensure `requirements.txt` is up-to-date with all necessary packages
like `fastapi`, `uvicorn`, `python-dotenv`, `ollama`, `pymongo`, `fastmcp`):

        ```bash

        pip install -r requirements.txt

        ```

    *   Create a `.env` file in the `backend` directory. This is where the backend and its managed MCP
services will look for environment variables:

        ```dotenv

        # For Web Search (server_search.py)

        SERPER_API_KEY=your_serper_api_key_here



        # For MongoDB (main.py)

        MONGODB_URI=mongodb://localhost:27017/

        MONGODB_DATABASE_NAME=mcp_chat_db



        # For MySQL Database Querying (server_mysql.py)

        DB_HOST=localhost

        DB_USER=your_db_user

        DB_PASSWORD=your_db_password

        DB_NAME=your_db_name



        # Optional: Default Ollama model if not specified by frontend or conversation

        # DEFAULT_OLLAMA_MODEL=qwen2:7b

        ```



3.  **Set up Frontend:**

    *   Navigate to the frontend directory:

        ```bash

        cd ../frontend

        ```

        (If you were in `backend/`, otherwise navigate from project root: `cd frontend`)

    *   Install Node.js dependencies:

        ```bash

        npm install

        # or

        # yarn install

        ```



4.  **Ensure Ollama is installed and a model is available:**

    The application might default to a specific model (e.g., `qwen2:7b`). Pull your desired model:

    ```bash

    ollama pull qwen2:7b

    # or your preferred model like llama3, mistral, etc.

    ```

    You can select the model in the UI if supported, or configure a default in the backend's `.env`
file.



## Usage



1.  **Ensure Prerequisites are Running:**

    *   **Ollama**: Must be running.

    *   **MongoDB**: Your MongoDB instance must be accessible.

    *   **MySQL Server** (if using the SQL tool): Your MySQL server must be running and accessible with
the credentials provided in `.env`.



2.  **Start the Backend Server:**

    Navigate to the `backend` directory and run the FastAPI application:

    ```bash

    # From the backend directory

    uvicorn main:app --reload --port 8000

    ```

    The backend API will typically be available at `http://localhost:8000`.

    The FastAPI application will automatically start and manage the MCP services (Web Search and SQL
Querying) as background processes using its lifespan manager. You do **not** need to run
`server_search.py` or `server_mysql.py` separately.



3.  **Start the Frontend Development Server:**

    Navigate to the `frontend` directory and run:

    ```bash

    npm run dev

    # or

    # yarn dev

    ```

    The web interface will typically be accessible at `http://localhost:5173` (or another port specified
by Vite).



### Interacting with the Application



-   Open your browser to the frontend URL (e.g., `http://localhost:5173`).

-   Use the chat interface to send messages.

-   Toggle "Use Web Search" or "Use Database" switches in the UI to enable these tools for your next
message.

-   Manage conversations using the sidebar (create new, select, rename, delete).



### Legacy Clients (Phasing Out)



The original `chat_client.py` (terminal) and `chat_frontend.py` (Gradio) are still in the repository but
are not part of the main refactored application. They are not maintained and may not work correctly with
the new backend.



## How It Works



1.  The **User** interacts with the **React Frontend**.

2.  The **Frontend** sends requests (chat messages, conversation management) to the **FastAPI Backend**
API.

3.  For chat messages, the **Backend** (`main.py`) processes the request:

    *   It may interact with **Ollama** to get a response from the local LLM.

    *   If the user enables a tool (e.g., web search, SQL query) via the UI:

        *   The backend prepares the necessary context (e.g., fetching database schema for SQL).

        *   It may prompt Ollama to generate a tool-specific input (e.g., a SQL query).

        *   The backend then communicates with the relevant **MCP Service Module** (managed as a
subprocess, e.g., `server_search.py` logic for web search, `server_mysql.py` logic for SQL execution).

        *   The MCP Service Module executes the tool (e.g., calls Serper.dev API, queries MySQL).

        *   Results from the MCP Service Module are returned to the Backend.

        *   The Backend may re-prompt Ollama with the tool's results to generate a final, informed
response.

4.  The **Backend** stores/retrieves conversation history from **MongoDB**.

5.  The final response is sent back to the **Frontend** and displayed to the user.



## File Structure (Simplified)


ollama-chat-with-mcp/

â”œâ”€â”€ backend/

â”‚   â”œâ”€â”€ main.py             # FastAPI application, API endpoints, chat logic, MCP service management

â”‚   â”œâ”€â”€ server_search.py    # MCP server module for web search

â”‚   â”œâ”€â”€ server_mysql.py     # MCP server module for SQL queries

â”‚   â”œâ”€â”€ requirements.txt    # Python dependencies for backend

â”‚   â””â”€â”€ .env                # Environment variables for backend (SERPER_API_KEY, DB_, MONGODB_)

â”œâ”€â”€ frontend/

â”‚   â”œâ”€â”€ src/

â”‚   â”‚   â”œâ”€â”€ App.jsx         # Main React application component

â”‚   â”‚   â”œâ”€â”€ components/     # React UI components (ChatInput, ChatMessage, etc.)

â”‚   â”‚   â”œâ”€â”€ services/       # API interaction logic (api.js)

â”‚   â”‚   â””â”€â”€ main.jsx        # Entry point for React app

â”‚   â”œâ”€â”€ package.json        # Node.js dependencies and scripts

â”‚   â”œâ”€â”€ vite.config.js      # Vite configuration (if using Vite)

â”‚   â””â”€â”€ public/             # Static assets

â”œâ”€â”€ chat_client.py          # (Legacy) Terminal-based chat client

â”œâ”€â”€ chat_frontend.py        # (Legacy) Gradio web interface client

â”œâ”€â”€ README.md               # This file

â””â”€â”€ .env.example            # Project-level example environment variables (can be removed if
backend/.env is canonical)





## Customization



-   **Ollama Model**: Change the default Ollama model in `backend/.env` or select from available models
in the UI.

-   **Search Results**: Adjust the number of search results processed in `backend/main.py`.

-   **Database Schema Context**: Modify `MAX_TABLES_FOR_SCHEMA_CONTEXT` in `backend/main.py` to control
how many tables' schemas are sent to the LLM.

-   **Prompt Engineering**: Modify the system prompts sent to Ollama in `backend/main.py` to tailor
responses and SQL generation.

-   **Styling**: Customize the frontend appearance by modifying CSS files or styles within the React
components in `frontend/src/`.



## Contributing



Contributions are welcome! Please feel free to submit a Pull Request.



## License



This project is licensed under the MIT License - see the LICENSE file for details.
